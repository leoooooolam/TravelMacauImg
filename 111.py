# -*- coding: utf-8 -*-
"""111.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNTSI1NYCN6idn92M4A40PNFWefOnv78
"""

!pip install gradio

import os
import json
import gradio as gr
from openai import OpenAI

client = OpenAI(
    api_key="sk-94274f0090214eafa6e10550d8aa4bfd",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

def format_history(chat_history):
    """è½¬æ¢å†å²è®°å½•ï¼Œè¿‡æ»¤æ€è€ƒå†…å®¹"""
    messages = [{"role": "system", "content": "You are a helpful assistant."}]
    for msg in chat_history:
        messages.append({"role": "user", "content": msg[0]})
        if msg[1]:
            # æå–æœ€åå›å¤å†…å®¹ï¼ˆè¿‡æ»¤æ€è€ƒå†…å®¹ï¼‰
            final_reply = msg[1].split("æœ€ç»ˆå›å¤ï¼š")[-1].strip()
            messages.append({"role": "assistant", "content": final_reply})
    return messages

def predict(message, chat_history):
    """å¤„ç†åŒ…å«æ€è€ƒå†…å®¹çš„æµå¼å“åº”"""
    chat_history.append((message, ""))
    messages = format_history(chat_history[:-1])
    messages.append({"role": "user", "content": message})

    completion = client.chat.completions.create(
        model="qwen3-235b-a22b",
        messages=messages,
        stream=True,
        stream_options={"include_usage": True},
        extra_body={"enable_thinking": True},
    )

    full_response = ""
    full_reasoning = ""
    current_reasoning = ""
    for chunk in completion:
        chunk_data = chunk.model_dump()
        if not chunk_data.get('choices') or len(chunk_data['choices']) == 0:
          continue  # è·³è¿‡æ— æ•ˆçš„chunk
        delta = chunk_data['choices'][0]['delta']

        # è§£ææ€è€ƒå†…å®¹
        if delta.get('reasoning_content'):
            current_reasoning = delta['reasoning_content']
            full_reasoning += current_reasoning

        # è§£ææ­£å¼å›å¤å†…å®¹
        if delta.get('content'):
            full_response += delta['content']

        # æ„å»ºæ˜¾ç¤ºå†…å®¹
        display_text = ""
        if full_reasoning != '':
            display_text = "ğŸ§  æ€è€ƒè¿‡ç¨‹ï¼š\n" + full_reasoning
        if full_response:
            display_text += f"\nğŸ’¡ æœ€ç»ˆå›å¤ï¼š\n{full_response}"

        # æ›´æ–°èŠå¤©è®°å½•
        if current_reasoning or delta.get('content'):
            chat_history[-1] = (message, display_text)
            yield chat_history

        # é‡ç½®å½“å‰æ€è€ƒå†…å®¹ï¼ˆé¿å…é‡å¤æ˜¾ç¤ºï¼‰
        if delta.get('content'):
            current_reasoning = ""

# è‡ªå®šä¹‰æ ·å¼
css = """
.gr-chatbot {min-height: 600px;}
.gr-chatbot .assistant-message {white-space: pre-wrap;}
.dark .reasoning {color: #90CAF9;}
.reasoning {color: #1E88E5; font-style: italic;}
"""

with gr.Blocks(css=css) as demo:
    gr.Markdown("# <center>ğŸ¤– åƒé—®å¤§æ¨¡å‹æ™ºèƒ½å¯¹è¯ï¼ˆå«æ€è€ƒè¿‡ç¨‹ï¼‰</center>")

    with gr.Row():
        chatbot = gr.Chatbot(
            bubble_full_width=False,
            render_markdown=True,
        )

    with gr.Row():
        msg = gr.Textbox(
            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
            container=False,
            scale=5
        )
        clear = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", scale=1)

    msg.submit(predict, [msg, chatbot], chatbot)
    clear.click(lambda: None, None, chatbot, queue=False)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0",debug=False)